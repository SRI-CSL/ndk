from __future__ import print_function

import csv
import sys
import math
from math import sqrt, factorial
import ndk
import numpy as np
from scipy.signal import convolve, butter, lfilter, decimate
import scipy.special
from ndk.ui import iprint,wprint,eprint
import ndk.ui as ui

# Features submodule for ndk - these are features derived from
# waveforms or LFPs.  Each feature is really a numeric quantity that
# helps to describe a relevant event (usually an action potential).
#
# The idea here is to provide an API that simplifies the addition of
# features to an existing table.
#
# Feature extraction is a dimensionality reduction that captures
# information that is most relevant to objectives (e.g., "parsing the
# neurogram").
#
# Feature extraction is the first step before, say, clustering or
# classification.

# Generally, the variable 'es' is used to hold the event store object
# that takes care of "electrical events", a.k.a. spikes.



################################################################
#
# Support for representing AP waveforms as linear combinations of an
# orthonormal basis.  Here, the basis is generated by a Gaussian with
# specified width and window size in samples, G0.  From there, we
# estimate the N successive derivatives of G0 and apply Gram-Schmidt
# to the set.  The result is an orthonormal basis set onto which any
# waveform can be projected to obtain coefficients.  In practice, 5-7
# coefficients provide a good fit to 64-sample waveforms, so the
# compression rate is high.

#
# Use Gaussians and their derivatives for a two-stage approximation
# for LFP waveforms:
#
# First, select a variance and a maximum "level" that are to be used
# to generate a sequence of orthogonal gaussians G_k (the gaussian and
# its derivatives).
#
# Second, compute coefficients using the inner product of the waveform
# window with {G_k}

#
# Note: J. Neural Engineering article by Yoo et al. (Grill is a
# co-author) has comparisons of A, B, and C fiber components that will
# have an impact on the kernels we wish to use here (sigma and width):


# In principle, other kernels can be used, but this is "spike-like":

# Note that the arguments here are in units of SAMPLES!

def gauss_kernel(width=64, sigma=20, offset=0, deriv=0):
    """Compute a gaussian kernel vector with the specified width and standard deviation (sigma)."""
    # This will produce the orthonormal set of Gaussians and their
    # derivatives using a closed form solution:
    w = int(width)

    # Array of kernel values:
    ker = np.zeros(w, dtype=np.float32)
    z = (0.5 * width) + offset
    sqrt2 = sqrt(2.0)

    f = 0.0    # Normalization factor
    if deriv > 0:
        s = 1.0 / sqrt((2**deriv) * factorial(deriv) * sqrt(2.0 * math.pi))
        h = scipy.special.hermite(deriv)
    else:
        # Gaussian derivatives are expressed in terms of the
        # "physicists'" Hermite polynomials:
        s = 1.0 / sqrt(2.0 * math.pi)
        h = lambda x: 1.0

    # Evaluate the requested derivative at each sample point:
    for i in range(0, w):
        x = (float(i) - z) / sigma
        ker[i] = s * h(sqrt2*x) * np.exp( -x*x )
        f += ker[i]*ker[i]

    # Normalize.  The resulting set will be orthonormal and is
    # suitable for use as a representation basis set:
    f = math.sqrt(f)
    for i in range(0, w):
        ker[i] = ker[i] / f

    return ker





def gen_squarewave(m, pulse, amplitude=1.0, bipolar=True):
    """Construct and return a square waveform array of length m samples
and pulse width 'pulse' samples."""
    result = np.zeros(m)
    p2 = int(0.5 * pulse)
    result[0:p2] = amplitude
    if bipolar:
        result[p2:pulse] = -amplitude
    else:
        result[p2:pulse] = amplitude
    return result


def sq_kernel(width=64, duration=20, offset=0, deriv=0):
    """Compute a square wave kernel vector with the specified width and duration (sigma)."""
    # This will produce the orthonormal set of Gaussians and their
    # derivatives using a closed form solution:
    w = int(width)
    print(f"w = {w}")
    # Array of kernel values:
    ker = np.zeros(w, dtype=np.float32)
    z = (0.5 * width) + offset

    # Fit square waves into this interval:
    h = 0.5 * duration
    i0 = int(z - h)
    i1 = int(z + h + 1)

    a = -1
    b = -1
    nflips = deriv + 1
    ns = int( (i1 - i0) / nflips )
    i = i0
    for k in range(nflips):
        for j in range(ns):
            ker[i] = b
            i += 1
        b = b * a

    return ker

def sq_kernel0(width=64, duration=20, offset=0, deriv=0):
    """Compute a square wave kernel vector with the specified width and duration (sigma)."""
    # This will produce the orthonormal set of Gaussians and their
    # derivatives using a closed form solution:
    w = int(width)
    print(f"w = {w}")
    # Array of kernel values:
    ker = np.zeros(w, dtype=np.float32)
    z = (0.5 * width) + offset

    i0 = int(z - duration)
    i1 = int(z + duration)

    a = -1
    b = -1
    n = (deriv+1)
    k = (i1 - i0) / n
    for i in range(i0, i1):
        j = i % k
        if n > 1 and j == 0:
            b = b * a
        ker[i] = b

    return ker



#
# Estimate the 1st derivative of the signal vector using a 3-point
# difference:
#
def deriv(signal):
    """Compute the 1st derivative of the supplied signal vector."""
    n = len(signal)
    ker = np.zeros(3)
    ker[0] =  0.5
    ker[2] = -0.5
    # print(signal)
    r = convolve(signal, ker)
    d = r[1:len(r)-1]
    m2 = np.dot(d,d)
    if m2 > 0.0:
        d = d / math.sqrt(np.dot(d,d))
    return d

# Generate the gaussian and its n-1 derivatives using the closed form
# resipe, i.e. H_i * exp(-ax^2) where H_i is the i-th Hermite
# polynomial:

#def gauss_deriv_sequence(n):
    

# Pass 1: Compute the "raw" sequence as G0 and its successive
# derivatives:

def kernel_sequence(sigma, width=64, ncomp=5, offset=0):
    """Compute an initial approximation to an orthonormal sequence of
functions, based on the Gaussian and its derivative, using the
specified standard deviation, width (default 64) and number of
components (default 5)."""
    result = []
    g0 = gauss_kernel(width, sigma, offset)
    result.append(g0)
    for i in range(0,ncomp-1):
        # We are going to use g0 to store successive derivatives, and
        # copy into g to create new vectors to populate the set:
        g0 = gauss_kernel(width, sigma, offset, i+1)
        g = np.zeros( int(width) )
        g[:] = g0[:]
        result.append(g)
    return result



# Gram-Schmidt projection operator:
def proj(a,b):
    """This function implements the projection step for Gram-Schmidt."""
    return b * (np.dot(a,b) / np.dot(b,b))

#
# From a sequence of gaussian derivatives, perform Gram-Schmidt on the
# sequence to produce an orthonormal basis (we should always do this
# to ensure an orthonormal basis):
#
def gs_sequence(seq):
    """Use Gram-Schmidt to turn the specified basis sequence (seq) into an
orthonormal basis set."""
    # u will be the return value (an orthonormal sequence):
    u = []
    for i in range(len(seq)):
        x = seq[i]
        for uk in u:
            x -=  proj(seq[i], uk)
        u.append(x)

    for vec in u:
        d = math.sqrt(np.dot(vec,vec))
        # print( d )
        vec[:] = vec[:] / d

    return u


#
# Sanity check:
#
def check_orth(seq, tol=1.0e-04, verbose=True):
    """Check the orthogonality of the basis sequence *seq*."""
    orth = True
    if verbose: iprint('[I] Orthonormality check:')
    i = 0
    rowvec = np.zeros(len(seq))
    for s1 in seq:
        if verbose: 
            with ui.context(ui.colors.OKGREEN) as c:
                print('[I] row {}:'.format(i), end="")
        j = 0
        for s2 in seq:
            v = np.dot(s1,s2)
            rowvec[j] = v
            if i == j and abs(v-1.0) > tol:
                orth = False
            if i != j and abs(v) > tol:
                orth = False
            j += 1
        if verbose:
            with ui.context(ui.colors.OKGREEN) as c:
                for v in rowvec:
                    print( "{0:5.2f} ".format(v), end="")
                print(' ')
        i += 1
    if verbose:
        if orth:   iprint("Basis is orthonormal.")
        else:      eprint("Basis is NOT orthonormal")
    return orth


def sqkernel_sequence(duration, width=64, ncomp=5, offset=0):
    """Compute an initial approximation to an orthonormal sequence of
functions, based on the Gaussian and its derivative, using the
specified standard deviation, width (default 64) and number of
components (default 5)."""
    result = []
    g0 = sq_kernel(width=width, duration=duration, offset=offset)
    result.append(g0)
    for i in range(0,ncomp-1):
        # We are going to use g0 to store successive derivatives, and
        # copy into g to create new vectors to populate the set:
        g0 = sq_kernel(width=width, duration=duration, offset=offset, deriv=i+1)
        g = np.zeros( int(width) )
        g[:] = g0[:]
        result.append(g)
    return result



# The defaults here work well for a width of 64:
def sqbasis(duration=10.0, width=64, nlevels=5, offset=0):
    """Construct a basis sequence with the desired standard deviation
(sigma), the specified width, and the number of components (basis
functions)."""
    # print( "In ndk.features.basis, width = {}".format(width) )
    s0 = sqkernel_sequence(duration, width, nlevels, offset)
    # s1 = gs_sequence(s0)
    check_orth(s0)
    #s1 = gs_sequence(s0)
    return s0

def square_wave_basis(duration, width, nfuncs):
    return sqbasis(duration, width, nfuncs)


#
# The defaults here work well for a width of 64:
def basis(sigma=10.0, width=64, nlevels=5, offset=0):
    """Construct a basis sequence with the desired standard deviation
(sigma), the specified width, and the number of components (basis
functions)."""
    # print( "In ndk.features.basis, width = {}".format(width) )
    s0 = kernel_sequence(sigma, width, nlevels, offset)
    # s1 = gs_sequence(s0)
    check_orth(s0)
    #s1 = gs_sequence(s0)
    return s0


def gaussian_basis(sigma, width, nfuncs):
    return basis(sigma, width, nfuncs)

# Designed for 'pre' files, or any situation involving spike-triggered
# capture.  This usually results in an asymmetric window around the
# spike, hence there is an offset on the Gaussian:
def pre_basis(sigma, width, nfuncs):
    return basis(sigma, width, nfuncs, -5)
        
# Takes a waveform window as obtained from an LFP recording, and a
# kernel_sequence as defined above.  Returns one coefficient per
# element of seq:
def waveform_coefs(waveform, seq):
    """Given a waveform window (a piece of the signal) and a sequence of
basis functions (seq), return a vector of coefficients obtained from
the inner product of the waveform with the basis set."""
    result = []
    for fn in seq:
        result.append(np.dot(waveform, fn))
    return result

#
# Reconstruct a waveform from the coefficients, and return the result
# as a vector of samples:
def gen_waveform(coefs, seq):
    """Reconstruct a waveform from the coefficients and the bssis sequence
(seq), and reults the resulting array."""
    m = len(seq[0])
    result = np.zeros(m)
    n = min(len(coefs), len(seq))
    for i in range(n):
        fn = seq[i]
        result += coefs[i] * fn
    return result




################################################################
#   Not sure how useful this will be...
#
# Experiment in spike "deconvolution" - assume that every compound
# action potential that we see in a waveform window is really a linear
# combination of "atomic" component spikes characterized by an
# amplitude and a discrete time offset.
#
# Construct a matrix A whose rows are copies of the atomic spike but
# with different delays.  A signal s is a linear combination s = A v
# where v is a vector of amplitudes.  Then v describes the "right"
# mixture of atomic spikes to get s.  The (pseudo)inverse of A applied
# to signal s will then yield a signature v that "explains" s in terms
# of our linear model.
#
# But keep in mind that this is simply a convolution.
#
# This is one step removed from a more rigorous phenomenological model
# that accounts for the filtering effects of vagus tissue.

# Construct A by copying the waveform:

def spike_matrix(waveform):
    """Returns a convolution matrix for the given waveform vector."""
    n = len(waveform)
    a = np.zeros((n,n))
    off = n/2
    for i in range(0,n):
        for j in range(0,n):
            k = i + j - off
            if k < 0:
                a[i,j] = waveform[0]
            elif k >= n:
                a[i,j] = waveform[n-1]
            else:
                a[i,j] = waveform[k]
    return a


################################################################

# The following set of functions compute macro properties from event
# stores over a given channel, e.g., means, maxes and mins, etc...

## UPDATE ME:
#
# Computes a mean waveform over all spikes for the given channel,
# subject to the time constraints, where 'dt' refers to the
# peak-to-trough time:
#
def mean_waveform(es, channel, min_dt=5, max_dt=10):
    """Compute a mean waveform for the specified channel and waveforms within the specified peak-to-trough time deltas."""
    n = ndk.spike_count(es)
    w = es.get_spike_waveform(0, channel)
    v = np.zeros(len(w))

    k = 0
    for i in range(0,n):
        da, dt = waveinfo(es, i, channel)
        if dt >= min_dt and dt <= max_dt:
            next = np.asarray(ndk.get_waveform(es, 0, channel))
            next -= next.mean()
            v += next
            k += 1

    if k > 0:
        v /= float(k)

    return v

#
# Returns a 2D array containing two values per channel:
#  1) peak-to-trough amplitude difference
#  2) Time delay from peak to trough
#
# The idea is to characterize the waveform in terms of its dynamic
# range and width.
#
def waveinfo(es, i, channel):
    """Returns 4 values:  peak-to-trough amplitude difference, peak-to-trough time difference, min amplitude, and max amplitude for all events on the specified channel."""
    w = es.get_spike_waveform(i, channel)
    min = 0
    max = 0
    tmin = 0
    tmax = 0
    for j in range(0,len(w)):
        if w[j] > max:
            max = w[j]
            tmax = j
        if w[j] < min:
            min = w[j]
            tmin = j
    return (max-min), (tmin-tmax), min, max


#
# Min and Max waveform amplitude, with peak-trough difference and
# peak-trough delta time:
#
def amp_min_max(es, channel):
    """For each spike, compute min, max, delta amplitude, and peak-to-trough time.  Return as an N x 4 array where N is the spike count."""
    n = es.get_spike_count()
    res = np.zeros((n,4))
    for i in range(0,n):
        da,dt,lo,hi = waveinfo(es, i, channel)
        res[i,0] = lo
        res[i,1] = hi
        res[i,2] = da
        res[i,3] = dt
    return res


#
# Support for mean firing rate - start with interspike interval and
# average from there:
#

def isi(es, channel):
    """Returns an array of interspike intervals for all events (channel is ignored and will be dropped soon)."""
    n = es.get_spike_count()
    isi = np.zeros(n)
    samples_per_second = es.sample_rate
    es.execute('select samplenum from spiketimes order by samplenum;')
    r = es.fetchall()
    isi[0] = 0
    for i in range(1,n):
        isi[i] = (r[i][0] - r[i-1][0]) / samples_per_second
    return isi



################################################################




def histogram(vector, nbins=100, min=None, max=None, clip=True):
    """Given a vector, returns a histogram vector along with metadata
    about binning."""
    if min == None:
        min = vector.min()
    if max == None:
        max = vector.max()
    dx = (max-min) / nbins
    iprint( "# min={}, max={}, dx={}".format(min, max, dx) )
    kmax = nbins-1
    hist = np.zeros(nbins)
    for val in vector:
        k = int( (val-min)/dx )
        if k < 0:
            if not clip:
                hist[0] += 1
        elif k > kmax:
            if not clip:
                hist[kmax] += 1
        else:
            hist[k] += 1
    return hist
    
#
# Get spike feature data conditional on a cue (the event): We need to
# re-think this.  Many of the vagus datasets have an injection cue.
# The older striatum datasets have multiple behavioral event cues.
# SQL queries can handle a wide variety of conditional retrieval (and
# therefore analyses) of spike data.  Should the db handle this?

def cue(es):
    """Return a vector of length N (N = number of spike events) that is 0 up until a desired cue event (e.g., injection or behavioral event) and k (the event label) after that point."""
    nev = es.event_count()
    event_info = es.get_events()
    if nev < 1:
        wprint("Error: this file should have exactly 1 event.  It contains {}!".format(nev))
    else:
        event_time = event_info[0][0]
        event_code = event_info[1][0]
        n = es.get_spike_count()
        vec = np.zeros(n)
        state = 0
        for i in range(0,n):
            curr_time = es.get_spike_timestamp(i)
            if curr_time > event_time:
                vec[i] = event_code
        return vec


#
# This might be redundant - look in db3.py:
#
def timestamps(es):
    """Returns an array of spike timestamps (Deprecated.  See *db3*)."""
    n = es.get_spike_count()
    vec = np.zeros(n)
    for i in range(0,n):
        vec[i] = float(es.get_spike_timestamp(i))
    return vec


# Return a window of length winsize from a signal, starting at offset i:

def get_window(signal, i, winsize):
    """Given a signal array (one channel of a local field potential recording, for example), and an index i, return a vector of length *winsize* that contains the values in the signal array."""
    # This function is used to extract a window of raw signal data
    # from an array (signal):
    y = np.zeros(winsize)
    j0 = i - (winsize / 2)
    n = len(signal)
    # Not sure why we have to subtract 16:
    for k in range(winsize):
        i = j0 + k - 16
        if i >= 0 and i < n:
            y[k] = signal[i]
    return y

#
# A comma-separated list of a1:b1,a2:b1...  where 'an' is a
# coefficient index (from the basis), and 'bn' is the channel number:
def parse_coef_spec(string):
    """Parse a coefficient specification of the form 'a_1:b_1,a_2:b_2...' where *a_n* is a basis coefficient index, and *b_n* is a channel number.  Returns a list of coefficient-channel index pairs."""
    l = string.split(',')
    coef = []
    for pair in l:
        chanspec = pair.split(':')
        coef.append((int(chanspec[0]), int(chanspec[1])))
    return coef


#
# This function isn't quite right - adding features should be
# implemented perhaps by adding new columns to a "feature table", or
# by adding new tables to the event store (one table per 'feature').
#
# This version augments an existing array 'x' with a new column.
#
# Add a feature vector x to a base array of features 'base':
#
def add_feature(base, x):
    """Deprecated."""
    if base == []:
        m = x.shape[0]
        k = 0
    else:
        if base.shape[0] != x.shape[0]:
            raise ValueError('both args must have the same first dimension')
        m = base.shape[0]
        k = base.shape[1]

    n = k + x.shape[1]
    out = np.zeros((m,n))

    if base != []:
        out[:,:-1] = base

    out[:,k:] = x

    return out



# Histogram-based threshold selection: Assumes that "useful" spikes
# appear only in the top x% of signal values.  This needs to be
# validated and compared to RMS-error-based threshold selection.
#
# 1) Find the min and max values in array
# 2) Histogram the array
# 3) Find the threshold that eliminates the given fraction of histogram mass
#
def find_threshold_hist(array, nbins, fraction):
    """Given a score *array*, histogram the array values into *nbins* bins, and select a threshold that rejects *fraction* of the array values."""
    rmin = array.min()
    rmax = array.max()
    hist, edges = np.histogram(array, nbins)

    tot = np.sum(hist)
    dr = (rmax-rmin) / len(hist)

    tnum = fraction * tot
    k = 0
    thresh = rmin

    for i in range(0, len(hist)):
        k += hist[i]
        thresh += dr
        if (k > tnum): return thresh

    return thresh


def find_threshold_snr(array, pfa=0.01):
    """Given a score *array*, select a threshold that results in a false alarm probability of 1 percent."""
    mean = array.mean()
    std = array.std()
    f = 5*scipy.special.erfcinv(2*pfa)
    return mean + (std * f)



def smooth_lfp(sig, width=31, sigma=8.0):
    """Smooth the LFP signal using a Gaussian kernel of the specified width and standard deviation sigma."""
    ker = gauss_kernel(width, sigma)
    return convolve(sig, ker)


# Histogram-based threshold selection: Assumes that "useful" spikes
# appear only in the top x% of signal values.  This needs to be
# validated and compared to RMS-error-based threshold selection.
#
# 1) Find the min and max values in array
# 2) Histogram the array
# 3) Find the threshold that eliminates the given fraction of histogram mass
#
def find_threshold(array, nbins, fraction):
    rmin = array.min()
    rmax = array.max()
    hist, edges = np.histogram(array, nbins)

    tot = np.sum(hist)
    dr = (rmax-rmin) / len(hist)

    tnum = fraction * tot
    k = 0
    thresh = rmin

    for i in range(0, len(hist)):
        k += hist[i]
        thresh += dr
        if (k > tnum): return thresh

    return thresh

#
# You MUST filter out low freqs (e.g., using a Butterworth filter)
# and pass the filtered signal as sig:
#
def spike_peaks(sig, threshold):
    """Given an LFP signal *sig* and a *threshold*, isolate all peaks that are above threshold, by finding the zero-crossings of the first derivative of the smoothed signal.  Returns a vector of 1s and 0s such that the 1s identify spikes."""
    upvec = sig > threshold
    iprint( 'Peak vector computed.' )
    sys.stdout.flush()

    smoothed = smooth_lfp(sig, 31, 6.0)
    mean = smoothed.mean()
    iprint( 'Smoothed signal computed' )
    sys.stdout.flush()
    first = deriv(smoothed)
    iprint( 'First derivative computed' )
    sys.stdout.flush()
    zc = np.zeros(len(sig))

    numzc = 0
    numpts = len(zc)
    tick = numpts / 100
    for i in range(1,len(zc)-1):
        if ( upvec[i] ):
            if (first[i] == 0 or first[i]*first[i-1] < 0):
                zc[i] = sig[i]
                numzc += 1
        if ( i % tick == 0 ):
            print( '{}% ({})'.format(i/tick, numzc), end="")
            sys.stdout.flush()

    print( ' ' )
    iprint( 'Zero-crossings of first derivative computed.' )
    sys.stdout.flush()
    return zc

def find_feature_peaks(signal, basis_seq, nbins=500, p=0.98):
    npts = len(signal)
    l = np.zeros(npts)
    width = len(basis_seq[0])
    margin = width / 2
    k = 1
    for ker in basis_seq:
        y = convolve(signal, ker)[margin-1:-margin]
        print( "Basis {}...".format(k), end="" )
        sys.stdout.flush()
        l = l + y*y
        k += 1
    thresh = find_threshold( l, nbins, p )
    zc = spike_peaks( l, thresh )



def zero_mean(w):
    """Given a vector w, return a zero-mean version of it, i.e., subtract the mean from the signal."""
    x = np.zeros(len(w))
    x = w - w.mean()
    return x


#
# Return a "decimated" signal, i.e., average pairs of data points and
# return a new array that's half the size of the old one:
#

def downsample(array, q):
    x = scipy.signal.decimate(array, q)
    return x
    

#
# Use a Butterworth bandpass filter:
#

def butter_bandpass(lowcut, highcut, fs, order=9):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band', output='ba')
    return b, a

def butter_low(cut, fs, order=9):
    nyq = 0.5 * fs
    fcut = cut / nyq
    b, a = butter(order, fcut, btype='low', output='ba')
    return b, a

def butter_high(cut, fs, order=9):
    nyq = 0.5 * fs
    fcut = cut / nyq
    b, a = butter(order, fcut, btype='high', output='ba')
    return b, a

   
#
# Default that's "nice" for spikes, but it might be good to play with
# the band:
#
def butter_bandpass_filter(data, lowcut, highcut, fs, order=9):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    y = lfilter(b, a, data)
    return y

def butter_lowpass_filter(data, cutoff, fs, order=9):
    b, a = butter_low(cutoff, fs, order=order)
    y = lfilter(b, a, data)
    return y

def butter_highpass_filter(data, cutoff, fs, order=9):
    b, a = butter_high(cutoff, fs, order=order)
    y = lfilter(b, a, data)
    return y


#
# The actual signal filter function:
#

def butter_filter_spikes(signal, lowcut, highcut, samprate, order=9):
    if lowcut is None:
        if highcut is None:
            y = signal
        else:
            y = butter_lowpass_filter(signal, highcut, samprate, order)
    else:
        if highcut is None:
            y = butter_highpass_filter(signal, lowcut, samprate, order)
        else:
            y = butter_bandpass_filter(signal, lowcut, highcut, samprate, order)
    return y


