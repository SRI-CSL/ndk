#!/usr/bin/env python
from __future__ import print_function

import sys
import ndk.ds
import ndk.features
import ndk.es.db3 as db
import math
import argparse
import sqlite3
import quantities as pq
import numpy as np
from scipy.signal import convolve, decimate, fftconvolve, butter, lfilter
from uritools import urisplit
from ndk.ui import iprint,wprint,eprint
import ndk.ui as ui

# Non-Theano version

# TO DO:  CHECK ON RECORDING 9 - THE GPU VERSION LOOKS WONKY.

# mkspikes populates a database file with spike times and spike basis
# numbers.  These correspond to the 'spiketimes' and 'spikebasis'
# tables.  Uses the basis scoring function to localize peaks in the
# score and mark spikes.


# Suspect that non-local memory access patterns due to bit shuffling
# kill the FFT approach for huge vectors:
use_fft = True
old_way = False

parser = argparse.ArgumentParser(description='Populate an sqlite3 database file with spike timestamps.') 
parser.add_argument('files',  metavar='FILE', nargs=1, help='An sqlite3 database file')
parser.add_argument('-basisid', type=int, default=0, help='Basis ID.')
parser.add_argument('-channel', type=int, default=0, help='Base channel to use for spike extraction.')
parser.add_argument('-percent', type=float, default=98, help='Rejection percentage.  Th bottom p percent of spikes are rejected.  p=99.5 by default.')
parser.add_argument('-threshold', type=float, default=2.0, help='Threshold to use when -sort threshold is supplied.')
parser.add_argument('-decimate', type=int, default=0, help="Decimation factor, if desired")
parser.add_argument('-filter',  default="300:2000")
parser.add_argument('-overlap', dest='overlap', action='store_true', default=True, help='If provided, allow overlap in spike windows (default action).')
parser.add_argument('-positive', dest='positive', action='store_true', default=True, help='If provided, use positive scores for marking events (default action).')
parser.add_argument('-negative', dest='negative', action='store_true', default=False, help='If provided, use negative scores for marking events.')
parser.add_argument('-sort', type=str, default='peaks', help='Specify the spike sorting algorithm: score, peaks (default), or threshold.')
parser.add_argument('-plot', default=None, help="If supplied, output file for plotting event score function.")
parser.add_argument('-rebuild', dest='rebuild', action='store_true', help='If provided, rebuild the db file by resetting all tables.')
parser.add_argument('-shift',  type=int, default=None, help='If provided, shift the actual timestamp by the specified number of samples.')
parser.add_argument('-smooth_score', type=str, default=None, help='How to smooth the scoring function (width:sigma).  Default is no smoothing.')
parser.add_argument('-window', type=int, default=1048576, help='Window size in samples for piecewise processing.')
ns = parser.parse_args()

allow_overlap = ns.overlap
if ns.channel > 255:
    print("It's not possible at this time to use more than 255 channels!")
    quit()

def parse_filter_args(arg):
    [lowstr, highstr] = arg.split(':')
    if len(lowstr) == 0:
        lo = None
    else:
        lo = float(lowstr)

    if len(highstr) == 0:
        hi = None
    else:
        hi = float(highstr)

    return lo,hi

if ns.filter.find(':') == -1:
    filter = False
    iprint( "No filtering." )
    lo = 0
    hi = 0
else:
    filter = True
    lo,hi = parse_filter_args(ns.filter)
    iprint( "Filtering:  Low cutoff at {} Hz, high cutoff at {} Hz.".format(lo, hi) )

do_smoothing = False
score_w = None
score_s  = None
if ns.smooth_score is not None:
    x = ns.smooth_score.split(':')
    if len(x) == 2:
        do_smoothing = True
        score_w = int(x[0])
        score_s = float(x[1])

dbname = ns.files[0]

if dbname == None:
    eprint( "You must supply a database name." )
    exit()

dbobj = db.open_event_store(dbname)
if dbobj==None:
    eprint( "Data store "+dbname+" does not exist!" )
    quit()


samprate, nchannels, uristring = dbobj.get_metadata()
iprint( "sample rate = {}; nchannels = {}; URI = {}".format( samprate, nchannels, uristring ))

if ns.rebuild:
    # Drop these tables so that they are completely rebuilt:
    dbobj.drop('spiketimes')
    dbobj.drop('spikebasis')
    dbobj.drop('spikecoefs')
    dbobj.drop('spikelabels')

dbobj.make_spiketimes_table(dbobj)
dbobj.make_spikebasis_table(dbobj)

dsobj = ndk.ds.open(uristring, dbname)
t0 = dsobj.t0
t1 = dsobj.t1

if (filter):
    dsobj.filter_data(lo, hi)
    signal = dsobj.get_chunk(ns.channel, t0, t1)
else:
    # samprate already set
    signal = dsobj.get_signal(ns.channel)

# nchannels already set and not even used.

# Histogram-based threshold selection: Assumes that "useful" spikes
# appear only in the top x% of signal values.  This needs to be
# validated and compared to RMS-error-based threshold selection.
#
# 1) Find the min and max values in array
# 2) Histogram the array
# 3) Find the threshold that eliminates the given fraction of histogram mass
#
def find_threshold(array, nbins, fraction):
    rmin = array.min()
    rmax = array.max()
    hist, edges = np.histogram(array, nbins)
    #print(hist)

    with open('/tmp/mkspikes-hist.dat', 'w') as f:
        for x in hist:
            f.write('{}\n'.format(x))
            

    tot = np.sum(hist)
    dr = (rmax-rmin) / len(hist)

    tnum = fraction * tot
    k = 0
    thresh = rmin

    for i in range(0, len(hist)):
        k += hist[i]
        thresh += dr
        if (k > tnum): return thresh

    return thresh

def find_threshold_snr(array, pfa=0.01):
    mean = array.mean()
    std = array.std()
    f = 5*scipy.special.erfcinv(2*pfa)
    return mean + (std * f)


def smooth_lfp(signal, width=31, sigma=8.0):
    ker = ndk.features.gauss_kernel(width, sigma)
    #print(signal.shape)
    #print(ker.shape)
    iprint("Kernel: {}".format(ker))
    if len(signal.shape) == 2:
        return convolve(signal[:,0], ker, mode='same')
    else:
        return convolve(signal[:], ker, mode='same')

#
# Windowed scoring - nwin is the number of windows needed to cover the
# signal.  Compute scores over each subwindow and return the result -
# should have the same number of points as the input signal:

def score_signal_basis(signal, n, seq, wsize):
    # Full on basis-set scoring - sum the "energy" produced over a
    # window by the signal convolved with all basis functions, and use
    # that as a score:
    l_full = np.zeros((n))
    nwin = int(n / wsize)
    if n % wsize > 0:
        nwin += 1
    widx = 0
    old_percent = 0
    # For each window in signal:
    for j in range(nwin):
        percent = (100 * j) / nwin
        if percent > old_percent:
            old_percent = percent
            print('Convolving...{:>3}%\r'.format(int(percent)), end="")
        i0 = widx
        i1 = widx + wsize
        widx = i1 + 1
        if i1 > n:
            i1 = n
            wsize = i1-i0
        if ns.negative:
            s = -signal[i0:i1]
        else:
            s = signal[i0:i1]
        k = 1
        # l = np.zeros((wsize,1))
        l = np.zeros((wsize))
        # For each kernel, sum up the convolution^2:
        for ker in seq:
            hw = len(ker)
            # print('signal shape: {}'.format(signal.shape))
            if len(signal.shape) == 2:
                ker2 = ker.reshape((len(ker),1))
            else:
                ker2 = ker
            # print('kernel shape: {}'.format(ker2.shape))
            if use_fft:
                yr = fftconvolve(s, ker2)
                # y = fftconvolve(s, ker2, mode='same')
            else:
                # y = convolve(signal, ker2, mode='same', method='direct')[margin-1:-margin]
                # Can't simply window here.  We have to window outside the kernel loop.
                yr = convolve(s, ker2, method='direct')
                # y = convolve(s, ker2, mode='same', method='direct')
            # print( "Basis {}...".format(k), end="" )
            sys.stdout.flush()
            # Force y to fit in l:
            y = yr[hw-1:]
            if len(y) > len(l):
                y = y[1:]

            if ns.positive or ns.negative:
                l = l + y
            else:
                l = l + y*y
            k += 1
        l_full[i0:i1] = l[:]
    sys.stdout.write('\n')
    return l_full



def score_signal_peak(signal, n, seq, wsize):
    # Use ONLY the first basis function to score the dataset:
    l_full = np.zeros((n))
    nwin = int(n / wsize)
    if n % wsize > 0:
        nwin += 1
    widx = 0
    old_percent = 0
    #
    # You can pass in None for seq and a 3-point smoothing kernel will
    # be used.  Do this if you are pre-filtering the data.  If you are
    # not filtering, you can use first kernel function which should
    # behave like a low-pass filter (the else clause):
    if seq is None:
        ker = np.zeros(3)
        ker[0] = 0.25
        ker[1] = 0.5
        ker[2] = 0.25
    else:
        # This is a more "traditional" peak-finding algorithm, on the
        # assumption that the first element of the basis set is a
        # gaussian:
        ker = seq[0]
    hw = len(ker)
    if len(signal.shape) == 2:
        ker2 = ker.reshape((len(ker),1))
    else:
        ker2 = ker

    # For each window in signal:
    for j in range(nwin):
        percent = (100 * j) / nwin
        if percent > old_percent:
            old_percent = percent
            print('Convolving...{:>3}%\r'.format(int(percent)), end="")
        i0 = widx
        i1 = widx + wsize
        widx = i1 + 1
        if i1 > n:
            i1 = n
            wsize = i1-i0
        if ns.negative:
            s = -signal[i0:i1]
        else:
            s = signal[i0:i1]

        l = np.zeros((wsize))

        if use_fft:
            yr = fftconvolve(s, ker2)
        else:
            # Can't simply window here.  We have to window outside the kernel loop.
            yr = convolve(s, ker2, method='direct')

        sys.stdout.flush()
        # Force y to fit in l:
        y = yr[hw-1:]
        if len(y) > len(l):
            y = y[1:]

        if ns.positive or ns.negative:
            l = l + y
        else:
            l = l + y*y

        l_full[i0:i1] = l[:]
    sys.stdout.write('\n')
    return l_full


#
# Best used with score smoothing
def score_raw_thresholds(signal, n, wsize, thresh):
    l_full = np.zeros((n))
    nwin = int(n / wsize)
    if n % wsize > 0:
        nwin += 1
    widx = 0
    old_percent = 0
    # For each window in signal:
    for j in range(nwin):
        percent = (100 * j) / nwin
        if percent > old_percent:
            old_percent = percent
            print('Convolving...{:>3}%\r'.format(int(percent)), end="")
        i0 = widx
        i1 = widx + wsize
        widx = i1 + 1
        if i1 > n:
            i1 = n
            wsize = i1-i0
        if ns.negative:
            s = -signal[i0:i1]
        else:
            s = signal[i0:i1]

        # l = np.zeros((wsize,1))
        l = np.zeros((wsize))

        l = np.where(s>thresh, s, 0.0)
            
        sys.stdout.flush()

        l_full[i0:i1] = l[:]
    sys.stdout.write('\n')
    return l_full


#
# You MUST filter out low freqs (e.g., using a Butterworth filter)
# and pass the filtered signal as sig:
#
def spike_peaks(sig, threshold, smoothing_width=31, sigma=6.0):
    iprint('Threshold: {}'.format(threshold))
    upvec = sig > threshold
    iprint( 'Peak vector computed.' )
    sys.stdout.flush()

    iprint("Length of LFP signal = {}".format(len(sig)))

    if not do_smoothing:
        smoothed = sig
        smoothedb = sig
    else:
        smoothedb = smooth_lfp(sig, smoothing_width, sigma)
        n = int(smoothing_width / 2)
        smoothed = smoothedb[n:-n]

    iprint("Length of smoothed signal = {}".format(len(smoothed)))
    # mean = smoothed.mean()
    iprint( 'Smoothed signal computed' )
    sys.stdout.flush()
    first = ndk.features.deriv(smoothed)
    iprint( 'First derivative computed (length={})'.format(len(first)) )
    sys.stdout.flush()
    zc = np.zeros(len(sig))

    numzc = 0
    numpts = len(zc)
    tick = numpts / 100
    for i in range(1,len(first)-2):
        if ( upvec[i] ):
            # A bug (or quirk) of the derivative computation results
            # in a shift of 1 sample:
            j = i+2
            # If the first derivative is exactly 0 here, then mark it:
            if (first[j] == 0):
                zc[i] = sig[i]
                numzc += 1
            # Else if there is a sign change, choose which side to mark, then mark it:
            elif first[j]*first[j-1] < 0:
                if abs(first[j-1]) < abs(first[j]):
                    zc[i-1] = sig[i-1]
                    numzc += 1
                else:
                    zc[i] = sig[i]
                    numzc += 1
                        
        if ( i % tick == 0 ):
            print( 'Zero crossings: {:>3}% ({:>6})\r'.format(i/tick, numzc), end="")
            sys.stdout.flush()

    sys.stdout.write('\n')
    iprint( 'Zero-crossings of first derivative computed.' )
    sys.stdout.flush()
    return zc,smoothedb

#
# Compute the basis:
#
seq = dbobj.get_basis(ns.basisid)
width = len(seq[0])
threshp = ns.percent / 100.0

if ns.decimate > 0:
    while ns.decimate > 1:
        signal = decimate(raw_data, 10)
        ns.decimate = ns.decimate / 10


npts = len(signal)
l = np.zeros(npts)
margin = width / 2
# print( margin )
k = 1
print( "Convolving...\r", end="" )
sys.stdout.flush()

# 
if ns.sort == 'peaks':
    iprint( "Using peak detection to score peaks in the dataset.".format(len(seq)) )
    #l = score_signal_peak(signal, npts, seq, ns.window)
    l = score_signal_peak(signal, npts, seq, ns.window)
    iprint( "Scoring complete.  Selecting top {} % of peaks.".format(100.0-threshp*100.0) )
    shift = 0  # This *should* cause the timestamp to line up with the peak.
elif ns.sort == 'score':
    iprint( "Using {} basis components to score the dataset.".format(len(seq)) )
    l = score_signal_basis(signal, npts, seq, ns.window)
    iprint( "Scoring complete.  Selecting top {} % of peaks.".format(100.0-threshp*100.0) )
elif ns.sort == 'thresh' or ns.sort == 'threshold':
    iprint( "Using raw thresholds to score the dataset.")
    l = score_raw_thresholds(signal, npts, ns.window, ns.threshold)
print()


if ns.shift is None:
    shift = len(seq[0]) / 2
else:
    shift = ns.shift
iprint('Using a shift of {} samples.'.format(shift))

bw = len(seq[0])
nbins = 500
thresh = find_threshold(l, nbins, threshp)
zc,smoothed = spike_peaks( l, thresh, score_w, score_s )
last = len(zc)

if ns.plot:
    from_samp = 0
    to_samp = len(l)
    if ns.plot.find(':') == -1:
        plot_file = ns.plot
    else:
        x = ns.plot.split(':')
        plot_file = x[0]
        from_samp = int(x[1])
        if len(x) > 2:
            to_samp = int(x[2])
    with open(plot_file, 'w') as f:
        for i in range(from_samp,to_samp):
            f.write('{} {} {}\n'.format(signal[i], l[i], smoothed[i]))


# why 16??
#ts = t0 - 16
dt = ns.decimate
if dt == 0:
    dt = 1
i = 0
iprev = 0
spikeid = dbobj.max_spike_id()
printed = False
previd = 0
with ui.context(ui.colors.OKGREEN) as c:
    print("[I] Spike events: ", end="")
    while (i < last):
        # spikep = False
        spikep = (zc[i] != 0)

        if (spikep):
            # The timestamp is in units of SAMPLES.
            ts = t0 + i*dt + shift
            if ( allow_overlap or ( (i-iprev) > width ) ):
                spikeid += 1
                dbobj.set_spike_time(spikeid, ts)
                dbobj.set_spike_basisid(spikeid, ns.basisid)
                # Also save the spike channel.  We can display or
                # analyze selectively on the channel that gave rise to
                # the spike:
                l = ns.channel
                if ns.negative:
                    l = -l
                dbobj.set_spike_channel(spikeid, l)
                iprev = i
        i = i + 1
        if spikeid-previd >= 100:
            print( '{} '.format(spikeid), end="")
            sys.stdout.flush()
            previd = spikeid

cmd = ' '.join(sys.argv)
iprint("logging: {}".format(cmd))
dbobj.add_history('"{}"'.format(cmd))
dbobj.close()
