#!/usr/bin/env python
from __future__ import print_function

import sys
import ndk.ds
import ndk.features
import ndk.es.db3 as db
import math
import argparse
import sqlite3
import quantities as pq
import numpy as np
from scipy.signal import convolve, fftconvolve, butter, lfilter
from uritools import urisplit

# Non-Theano version

# mkspikes populates a database file with spike times and spike basis
# numbers.  These correspond to the 'spiketimes' and 'spikebasis'
# tables.  Uses the basis scoring function to localize peaks in the
# score and mark spikes.


# Suspect that non-local memory access patterns due to bit shuffling
# kill the FFT approach for huge vectors:
use_fft = False

parser = argparse.ArgumentParser(description='Populate an sqlite3 database file with spike timestamps.')
parser.add_argument('files',  metavar='FILE', nargs=1, help='An sqlite3 database file')
parser.add_argument('-basisid', type=int, default=0, help='Basis ID.')
parser.add_argument('-channel', type=int, default=0, help='Base channel to use for spike extraction.')
parser.add_argument('-percent', type=float, default=99.5, help='Rejection percentage.  Th bottom p% of spikes are rejected.  p=99.5% by default.')
parser.add_argument('-overlap', default="off", help='If "on", allow overlap in spike windows.')
parser.add_argument('-window', default=31, type=int, help='Smoothing window, in samples.')
parser.add_argument('-sigma', default=6.0, type=float, help='Smoothing sigma, in samples.')
parser.add_argument('-detect', default='leading', help='Check direction.  Default is "leading", i.e., detect the leading edge of the spike.  Other options are "trailing" or "both".')
parser.add_argument('-filter',  default="300:4000")
ns = parser.parse_args()

allow_overlap = (ns.overlap == "on")
if ns.filter.find(':') == -1:
    filter = False
    print( "No filtering." )
    lo = 0
    hi = 0
else:
    filter = True
    [lowstr, highstr] = ns.filter.split(':')
    lo = int(lowstr)
    hi = int(highstr)
    print( "Filtering:  Low cutoff at {} Hz, high cutoff at {} Hz.".format(lo, hi) )

dbname = ns.files[0]

if dbname == None:
    print( "You must supply a database name." )
    exit()

dbobj = db.open_event_store(dbname)
if dbobj==None:
    print( "Data store "+dbname+" does not exist!" )
    quit()


samprate, nchannels, uri = dbobj.get_metadata()
print( samprate, nchannels, uri )

dbobj.make_spiketimes_table(dbobj)
dbobj.make_spikebasis_table(dbobj)

smr = urisplit(uri)
recfile = smr.path
lfp_vec, samprate, t0, t1, seg = ndk.ds.read_raw_data(recfile)
nchannels = len(lfp_vec)

#
# Use a Butterworth bandpass filter:
#

def butter_bandpass(lowcut, highcut, fs, order=5):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a
   
   
#
# Default that's "nice" for spikes, but it might be good to play with
# the band:
#
def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    y = lfilter(b, a, data)
    return y


#
# Hate to use globals, but:
#

def butter_filter_spikes(signal, lowcut=lo, highcut=hi):
    y = butter_bandpass_filter(signal, lowcut, highcut, samprate)
    return y

# Histogram-based threshold selection: Assumes that "useful" spikes
# appear only in the top x% of signal values.  This needs to be
# validated and compared to RMS-error-based threshold selection.
#
# 1) Find the min and max values in array
# 2) Histogram the array
# 3) Find the threshold that eliminates the given fraction of histogram mass
#
def find_threshold(array, nbins, fraction):
    rmin = array.min()
    rmax = array.max()
    hist, edges = np.histogram(array, nbins)

    tot = np.sum(hist)
    dr = (rmax-rmin) / len(hist)

    tnum = fraction * tot
    k = 0
    thresh = rmin

    for i in range(0, len(hist)):
        k += hist[i]
        thresh += dr
        if (k > tnum): return thresh

    return thresh

def find_threshold_snr(array, pfa=0.01):
    mean = array.mean()
    std = array.std()
    f = 5*scipy.special.erfcinv(2*pfa)
    return mean + (std * f)


def smooth_lfp(signal, width=31, sigma=8.0):
    ker = ndk.features.gauss_kernel(width, sigma)
    return convolve(signal, ker)


#
# You MUST filter out low freqs (e.g., using a Butterworth filter)
# and pass the filtered signal as sig:
#
def spike_peaks(sig, threshold):
    upvec = sig > threshold
    print( 'Peak vector computed.' )
    sys.stdout.flush()

    smoothed = smooth_lfp(sig, ns.window, ns.sigma)
    mean = smoothed.mean()
    print( 'Smoothed signal computed' )
    sys.stdout.flush()
    first = ndk.features.deriv(smoothed)
    print( 'First derivative computed' )
    sys.stdout.flush()
    zc = np.zeros(len(sig))

    numzc = 0
    numpts = len(zc)
    tick = numpts / 100
    for i in range(1,len(zc)-1):
        if ( upvec[i] ):
            if (first[i] == 0 or first[i]*first[i-1] < 0):
                zc[i] = sig[i]
                numzc += 1
        if ( i % tick == 0 ):
            print( '{}% ({})'.format(i/tick, numzc), end="")
            sys.stdout.flush()

    print( ' ' )
    print( 'Zero-crossings of first derivative computed.' )
    sys.stdout.flush()
    return zc

#
# Compute the basis:
#
seq = dbobj.get_basis(ns.basisid)
width = len(seq[0])

p = ns.percent / 100.0

#print( "Using 5 basis components to score the dataset." )

signal = lfp_vec[ns.channel]


if (filter): 
    x = butter_filter_spikes(signal)
    signal = x

npts = len(signal)
l = np.zeros(npts)
margin = width / 2
# print( margin )
k = 1
print( "Convolving...", end="" )
sys.stdout.flush()

ker = np.zeros(3)
ker[0] = -1.0
ker[1] = 1.0

y = convolve(signal, ker)[1:-1]
if ns.detect == 'leading':
    print('Detecting leading edges.')
    l = -y
elif ns.detect == 'trailing':
    print('Detecting trailing edges.')
    l = y
else:
    print('Detecting all edges.')
    l = y*y
    

print()
print( "Scoring complete.  Selecting top {} % of peaks.".format(100.0-p*100.0) )

nbins = 500
thresh = find_threshold(l, nbins, p)
zc = spike_peaks( l, thresh )
last = len(zc)

# why 16??
#ts = t0 - 16
dt = 1
i = 0
iprev = 0
spikeid = -1
printed = False
previd = 0
while (i < last):
    # spikep = False
    spikep = (zc[i] != 0)

    if (spikep):
        # The timestamp is in units of SAMPLES.
        ts = t0 + i*dt
        if ( allow_overlap or ( (i-iprev) > width ) ):
            spikeid += 1
            dbobj.set_spike_time(spikeid, ts)
            dbobj.set_spike_basisid(spikeid, ns.basisid)
            iprev = i
    i = i + 1
    if spikeid-previd >= 100:
        print( '{} '.format(spikeid), end="")
        previd = spikeid

print( ' ' )
dbobj.close()
