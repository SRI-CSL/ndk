#!/usr/bin/env python

import sys
import argparse
import numpy
import ndk.es.db3 as db
import ndk.features
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

def pca_reduce(parray, ncomp=3):
    p = PCA(n_components=ncomp)
    pf = p.fit(parray)
    return pf.transform(parray)

#
# Relabel a spike file directly using KMeans:
#

def kmeans_cluster(x, n_clusters=4):
    p = KMeans(n_clusters=n_clusters)
    pf = p.fit(x)
    return pf.predict(x)


def kmeans_relabel(ds, coefs, ncomp=2, nclust=4):
    vals = ds.get_coef_array(coefs)
    pwr = vals.T

    if (ncomp > 0):
        x = pca_reduce(pwr, ncomp)
    else:
        x = pwr

    l = kmeans_cluster(x, nclust)
    print( l )
    ds.clear_spike_labels()
    for i in range(0,len(l)):
        ds.set_spike_label(i, l[i])


parser = argparse.ArgumentParser(description='Performs good old K-means clustering on the dataset.')
parser.add_argument('files',  metavar='FILE', nargs=1, help='An sqlite3 database file')
parser.add_argument('-coefs', default='0:0,0:1,0:2', help='Coefficients to use for clustering.')
parser.add_argument('-ncomp', type=int, default=3, help='Number of PCA components to use for dimensionality reduction.')
parser.add_argument('-nclusters', type=int, default=5, help='Number of clusters to try.')
ns = parser.parse_args()

filename = ns.files[0]

ncomp = ns.ncomp
nclusters = ns.nclusters
coefs = ndk.features.parse_coef_spec(ns.coefs)
dbobj = db.open_event_store(filename)

if dbobj != None:
    kmeans_relabel(dbobj, coefs, ncomp, nclusters)
    h = dbobj.label_histogram()
    print( 'Label counts:' )
    for i in range(len(h)):
        print( 'label {}: {}'.format(i, h[i]) )
    dbobj.close()


