#!/usr/bin/env python

import sys
import ndk.ds
import ndk.features
import ndk.es.db3 as db
import math
import argparse
import sqlite3
import numpy as np
from math import sqrt
from ndk.ui import iprint,wprint,eprint
from sklearn.decomposition import PCA

# Performs PCA on spike waveforms within a single recording.  Basic
# idea: Walk through an event source (.db file) collecting spikes that
# correspond to a given basis.  This defines a fixed-length window for
# each discovered spike.  Reduce to zero-mean, normalize and then
# construct the matrix of inner products across pairs of spikes.

# See if the resulting principal components are useful for clustering.

parser = argparse.ArgumentParser(description='PCA over raw data waveforms.')
parser.add_argument('files',  metavar='FILE', nargs=1, help='An sqlite3 database file.')
parser.add_argument('-channel', type=int, default=2, help='Channel to use')
parser.add_argument('-basis', type=int, default=0, help='Basis ID to use')
parser.add_argument('-filter',  default="100:3000")
parser.add_argument('-ncomp', type=int, default=5, help='Number of PCA components to use')
parser.add_argument('-pad', type=int, default=10, help='Padding to apply to the window on either side.  Widens the component.')
ns = parser.parse_args()


def parse_filter_args(arg):
    [lowstr, highstr] = arg.split(':')
    if len(lowstr) == 0:
        lo = None
    else:
        lo = float(lowstr)

    if len(highstr) == 0:
        hi = None
    else:
        hi = float(highstr)

    return lo,hi


if ns.filter.find(':') == -1:
    filtering = False
    iprint( "No filtering." )
    lo = 0
    hi = 0
else:
    filtering = True
    lo,hi = parse_filter_args(ns.filter)
    iprint( "Filtering:  Low cutoff at {} Hz, high cutoff at {} Hz.".format(lo, hi) )



def pca_reduce(parray, ncomp=3):
    p = PCA(n_components=ncomp)
    pf = p.fit(parray)
    return pf.transform(parray)


dbname = ns.files[0]

esobj = db.open_event_store(dbname)
samprate, nchannels, recfile = esobj.get_metadata()
dsobj = ndk.ds.open(recfile, dbname)

if filtering:
    dsobj.filter_data(lo, hi, 6)

esobj.execute('select spiketimes.spikeID,spiketimes.samplenum from spikebasis join spiketimes join spikechannels where spikebasis.spikeID=spiketimes.spikeID and spikechannels.spikeID=spiketimes.spikeID and spikebasis.basisID = {} and spikechannels.channel = {};'.format(ns.basis, ns.channel))
result = esobj.fetchall()

print("Select query returned {} results.".format(len(result)))

spiketimes = {}
waveforms = {}

seq = esobj.get_basis(ns.basis)
wlen = len(seq[0])
wh = ns.pad + wlen / 2
wlen += 2*ns.pad

print("Feature size = {} samples".format(wlen))
#
# Collect waveforms:
counter = 0
dc = len(result) / 100;
spike_ids = []

n = len(result)
print("Collecting {} waveforms...".format(n))
for r in range(0,n):
    spike_id = result[r][0]
    spike_ts = result[r][1]

    t0 = spike_ts-wh
    t1 = t0 + wlen
    spike_ids.append(spike_id)
    spiketimes[spike_id] = spike_ts
    wv = dsobj.get_chunk(ns.channel, t0, t1)
    if False:
        # Zero-mean:
        wv[:] -= wv.mean()
        # Normalized
        m = 0.0
        for i in range(len(wv)):
            m += wv[i]*wv[i]
        m = sqrt(m) 
        wv[:] /= m
    waveforms[spike_id] = wv

print("Sanity check: get_chunk returns windows of length {}".format(len(wv)))
wv_array = np.zeros((n, wlen))
i = 0
for id in spike_ids:
    try:
        wv_array[i,:] = waveforms[id]
        i += 1
    except:
        print("Spike {} shape mismatch.".format(id))

print("Invoking PCA...")
p = PCA(n_components=ns.ncomp)
pf = p.fit(wv_array)
tf = pf.transform(wv_array)

print("Setting new coefficients...")
for k in range(n):
    coefs = tf[k]
    esobj.set_spike_coefs(spike_ids[k], ns.channel, coefs)

esobj.commit()

esobj.execute("select max(basisID) from basis")
x = esobj.fetchone()
new_basis_id = x[0] + 1
print("Creating new PCA basis with ID = {}".format(new_basis_id))
esobj.add_basis(new_basis_id, 8.0, wlen, nfuncs=ns.ncomp, name='')
fn_list = [p.components_[i] for i in range(ns.ncomp)]
esobj.set_functions(new_basis_id, fn_list)
print("Setting spike basis IDs...")
for id in spike_ids:
    esobj.set_spike_basisid(id, new_basis_id)

esobj.commit()

print('transformed shape: {}'.format(tf.shape))
print('component shape: {} '.format(p.components_.shape))
print('explained variance ratios: {}'.format(p.explained_variance_ratio_))
print("Emitting component {}-vectors".format(wlen))
with open('/tmp/coefs.dat', 'w') as f:
    for i in range(wlen):
        for j in range(ns.ncomp):
            f.write("{} ".format(p.components_[j,i]))
        f.write('\n')



